{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск\n",
    "Градиентные методы - общий подход для обучения моделей (не всегда возможно решить задачу аналитически)\n",
    "\n",
    "\n",
    "Антиградиент указывает в сторону наискорейшего убывния функции. \n",
    "\n",
    "Это можно использовать для поиска минимума функции (минимизация функционала ошибки, например, MSE).\n",
    "\n",
    "### Алгоритм\n",
    "1. Стартуем из случайной точки\n",
    "2. Сдвигаемся по антиградиенту\n",
    "3. Повторяем, пока не окажемся в точке минимума\n",
    "\n",
    "\n",
    "### Пример\n",
    "Один признак\n",
    "\n",
    "Модель:\n",
    "$$a(x)=w_1 x + w_0$$\n",
    "\n",
    "Два параметра: $w_0$ и $w_1$\n",
    "\n",
    "Функционал ошибки MSE:\n",
    "$$\\displaystyle {Q(w_0, w_1) = \\frac{1}{l} \\sum _{i=1}^{l}{(w_1 x_i + w_0 - y_i)^2}}$$\n",
    "\n",
    "<img src='images/MSE.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle {Q(w_0, w_1) = \\frac{1}{l} \\sum _{i=1}^{l}{(w_1 x_i + w_0 - y_i)^2}}$$\n",
    "Счиаем частные производные по каждому из весов:\n",
    "$$\\frac {\\partial Q}{\\partial w_1} = \\frac {2}{l} \\sum _{i=1} ^{l} x_i (w_1 x_i + w_0 -y_i)$$\n",
    "\n",
    "$$\\frac {\\partial Q}{\\partial w_0} = \\frac {2}{l} \\sum _{i=1} ^{l} (w_1 x_i + w_0 -y_i)$$\n",
    "\n",
    "Градиент (вектор с двумя компонентами):\n",
    "\n",
    "$$\\nabla Q(w) = (\\frac {2}{l} \\sum _{i=1} ^{l} x_i (w_1 x_i + w_0 -y_i), \\frac {2}{l} \\sum _{i=1} ^{l} (w_1 x_i + w_0 -y_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Алгоритм градиентного спуска Full GD\n",
    "### Начальное приближение\n",
    "Сначала нужно как-то инициализировать веса ($w^0$, 0 - номер итерации). Можно сгенерировать веса из стандартного нормального распределения. \n",
    "\n",
    "### Шаг алгоритма (повторять до сходимости)\n",
    "$$w^t = w^{t-1} - \\eta \\nabla Q(w^{t-1})$$\n",
    "- $t$ - номер итерации\n",
    "- $w^t$ - новая точка\n",
    "- $\\eta$ - размер шага\n",
    "- $\\nabla Q(w^{t-1})$ - градиент в предыдущей точке\n",
    "\n",
    "Остановить процесс, если:\n",
    "- вектор весов почти не меняется $||w^t - w^{t-1}|| < \\epsilon$\n",
    "или \n",
    "- если норма градиента близка к нулю $||Q(w^{t})|| < \\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить 2 графика (именения w0 w1) и прямая по точкам"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если признаков много:\n",
    "$$\\displaystyle {Q(w) = \\frac{1}{l} \\sum _{i=1}^{l}{(<w, x> - y_i)^2}}$$\n",
    "\n",
    "$$\\frac {\\partial Q}{\\partial w_1} = \\frac {2}{l} \\sum _{i=1} ^{l} x_{i1} (<w, x> -y_i)$$\n",
    "$$\\frac {\\partial Q}{\\partial w_d} = \\frac {2}{l} \\sum _{i=1} ^{l} x_{id} (<w, x> -y_i)$$\n",
    "\n",
    "$$\\nabla Q(w) = \\frac {2}{l} X^T (Xw -y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проблема локальных минимумов\n",
    "- Локальный минимум - точка, в некоторой окрестности которой нет более маленьких значений\n",
    "- Глобальный минимум - самая низкая точка функции\n",
    "- Если функция выпуклая (например MSE), то у неё один глобальный=локальный минимум\n",
    "- Другие функции потерь могут не быть выпуклыми\n",
    "<img src='images/local_min.png'>\n",
    "- Цель - найти глобальный минимум (там меньше ошибка модели), но это не всегда возможно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Если стартуем из неудачной точки, то находим только локальный минимум (и застреваем там)\n",
    "<img src='images/local_min2.png'>\n",
    "- Поэтому градиентный спуск находит __локальный минимум__\n",
    "- Можно использовать мулитистарт (запуск градиентного спуска из разных начальных точек)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/local_min3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Длина шага\n",
    "$$w^t = w^{t-1} - \\eta \\nabla Q(w^{t-1})$$\n",
    "$\\eta$ позволяет контролировать скорость обучения\n",
    "- Если сделать длину шага недостаточно маленькой, градиентный спуск может разойтись\n",
    "Длина шага - параметр, который нужно подбирать\n",
    "\n",
    "Линия уровня - кривая, вдоль которой функция принимает одно и то же значение. Вид сверху на параболоид\n",
    "\n",
    "Вектор градиента перепендикулярен линии уровня\n",
    "\n",
    "#### Переменная длина шага\n",
    "$$w^t = w^{t-1} - \\eta _t \\nabla Q(w^{t-1})$$\n",
    "Длину шага можно менять в зависимости от шага\n",
    "Например, $$\\eta _t = \\frac{1}{t}$$. Чем больше итераций сделано, тем меньше шаг\n",
    "Или $$\\eta _t = \\lambda(\\frac{s}{s+t})^p$$ Дополнительные параметры\n",
    "\n",
    "#### Масштабирование признаков\n",
    "- Алгоритм может разойтись, если признаки имеют разные масштабы (линии уровня вытянутые), эллипс, а не круг\n",
    "\n",
    "$$x_i ^j = \\frac{x_i ^j - \\mu _j}{\\sigma_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стохастический градиентный спуск SGD\n",
    "Для вычисления обычного градиента, для каждой частной производной происходит суммирование по всей обучающей выборке. Она может быть очень большой. Это нужно делать для каждого шага (на каждой итерации).\n",
    "Для этого требуется слишком много вычислений. \n",
    "\n",
    "Будем шагать не по среднему по всем градиентам, а по градиенту одного объекта (одного слагаемого из функционала ошибки).\n",
    "\n",
    "## Алгоритм стохастического градиентного спуска\n",
    "\n",
    "1. Начальное приближение $w^0$\n",
    "2. Шаг алгоритма \n",
    "Повторять, каждый раз выбирая случайных объект $i_t$\n",
    "$$w^t = w^{t-1} - \\eta \\nabla L(y_{i_{t}}, a(x_{i_{t}}))$$\n",
    "Берём значение функции потерь на объекте $i_t$\n",
    "\n",
    "3. Остановить процесс, если вектор весов почти не меняется $||w^t - w^{t-1}|| < \\epsilon$\n",
    "\n",
    "Траектория стохастического градиентного спуска будет более ломанной, и в конце будет сложно попасть в минимум. Здесь очень важная уменьшающаяся длина шага\n",
    "\n",
    "Для сходимости SGD требуется гораздо больше итераций, чем для Full GD, но каждый шаг очень быстрый"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch SGD\n",
    "То же самое, что и SGD, но шаг делается по нескольким случайным объектам (batch, пакет)\n",
    "\n",
    "SGD можно применять на больших выборках, не помещающихся в RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Другие модификации GD\n",
    "###  Проблемы градиентного спуска:\n",
    "1. Сложные линии уровня (если линии уровня имеют форму эллипса, первые шаги будут выглядеть как осцилляции (колебания), а не движение в направлении к минимуму). Будет трактория зигзага и много лишних итераций. Чем меньше линии уровня похожи на окружности, тем сложнее градиентному спуску двигаться. Это может произойти из-за немасштабированных признаков или сложной функции потерь.\n",
    "2. Разная скорость сходимости по разным параметрам\n",
    "### Градиентный спуск с инерцией (momentum)\n",
    "$$h_t = \\alpha h_{t-1} + \\eta _t \\nabla Q (w^{t-1})$$\n",
    "\n",
    "$$w^t = w^{t-1} - h_t$$\n",
    "\n",
    "- $h_t$ - инерция, усредненное направление движения, обновляется на каждом шаге\n",
    "- $\\alpha$ - параметр затухания (обычно 0.9), гиперпараметр\n",
    "- $\\eta_t$ - длина шага\n",
    "- _Как будто шарик, который катится в сторону минимума, очень тяжёлый_\n",
    "- Копим в $h_t$ среднее значение градиента со всех прошлых шагов. Экспоненциально-затухающее среднее. Берём все прошлые значения, умножаем на $\\alpha$ и прибавляем новый градиент. Чем раньше был какой-то градиент, тем меньше он будет иметь вклад в $h_t$. \n",
    "- Метод очень популярен в глубоком обучении в задачах машинного зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Без инерции\n",
    "<img src='images/gd_momentum.png'>\n",
    "\n",
    "С инерцией\n",
    "<img src='images/gd_momentum2.png'>\n",
    "\n",
    "Осцилляции быстро убывают. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov momentum\n",
    "$$h_t = \\alpha h_{t-1} + \\eta _t \\nabla Q (w^{t-1} - \\alpha h_{t-1})$$\n",
    "\n",
    "$$w^t = w^{t-1} - h_t$$\n",
    "\n",
    "- $w^{t-1} - \\alpha h_{t-1}$ - неплохая оценка того, куда мы попадём на следующем шаге\n",
    "- шагаем в направлении $\\alpha h_{t-1}$, считаем градиент в этой точке и \"дошагиваем\" по этому антиградиенту дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проблема с разреженными данными\n",
    "Например, в модели есть категориальные признаки и использовалось one-hot кодирование. По редким бинарным признакам (редкая категория) шаги могут быть очень маленькими, так как объект с редкой категорией может встретитьсся уже ближе к концу работу алгоритма, когда шаги очень маленькие. Веса будут настраиваться хуже.\n",
    "\n",
    "#### Проблема с разным масштабом\n",
    "Какой-то признак может меняться в диапазоне от 0 до 1, а какой-то от 0 до 1 млн. Тогда нужно шагать по каждому параметру с разной скоростью. Быстрее обновлять веса при признаке единичного масштаба и медленнее при признаке миллионного масштаба. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "$$G^ t _j = G^ {t-1} _j + (\\nabla Q(w^{t-1}))^2 _j$$\n",
    "\n",
    "$$w^t _j = w^{t-1} _j - \\frac{\\eta _t}{\\sqrt{G^t _j + \\epsilon} } (\\nabla Q (w^{t-1}))_j$$\n",
    "\n",
    "- В $G_j$ накапливаются квадраты градиентов по $j$ признаку, то есть частных производных по нему. Насколько сильно уже нашагали по j параметру\n",
    "- Длина шага нормируется на знаменатель. Если по этому признаку много нашагали, то знаменатель большой. \n",
    "- По каждому параметру будет своя скорость\n",
    "- Недостаток алгоритма $G_j$ может только расти на каждой итерации t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "$$G^ t _j = \\alpha G^ {t-1} _j + (1-\\alpha)(\\nabla Q(w^{t-1}))^2 _j$$\n",
    "\n",
    "$$w^t _j = w^{t-1} _j - \\frac{\\eta _t}{\\sqrt{G^t _j + \\epsilon} } g_{tj}$$\n",
    "- Скорость зависит только от недавних шагов\n",
    "- $\\alpha$ обычно 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "Совмешает идею инерции и своей длины шага по каждому признаку\n",
    "\n",
    "// страшная формула"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге momentum позволяет избавиться от осцилляций, а методы типа AdaGrad позволяют грамотнее задавать темп движения по каждому параметру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
